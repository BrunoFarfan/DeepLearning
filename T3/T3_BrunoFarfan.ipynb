{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMfCWFpB06WN"
      },
      "source": [
        "\n",
        "#Tarea 3\n",
        "---\n",
        "*03-06-2023*\n",
        "\n",
        "*Martín Gallegos*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dam7QoCl7Iuw"
      },
      "source": [
        "# Identificación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtGVTWuj7KxD"
      },
      "outputs": [],
      "source": [
        "Nombre = \"Bruno Farfán\"\n",
        "id = \"2064230J\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preguntas Teóricas:\n",
        "\n",
        "## Pregunta 1:\n",
        "\n",
        "Que el problema sea de tipo Markoviano es importante, ya que nos permite resumir todo el entorno en el estado actual del modelo (ya que la probabilidad de ocurrencia de estados futuros solamente depende del estado actual), haciendo que el Actor pueda tomar una acción basandose exclusivamente en el estado actual, y con este obtener una imagen completa del mundo en el que se encuentra.\n",
        "Si el problema no fuese Markoviano, entonces el modelo tendría que tener en consideración los estados pasados para tomar una acción y aunque el algoritmo considera estocasticidad y por lo tanto tiene resultados futuros inciertos, como se mencionó, esta incertidumbre depende exclusivamente del estado actual y no de los estados pasados.\n",
        "\n",
        "## Pregunta 2:\n",
        "\n",
        "La función `Q(s,a)` nos entrega la recompensa de ejecutar la acción `a` cuando estamos en el estado `s`. Es decir, nos está diciendo que tan bueno o malo es tomar una acción `a` cuando nos encontramos en el estado `s`.\n",
        "Nos interesa estimarla, debido a que al decirnos que tan buena o mala es cierta acción en determinado estado, nos está dando una guía o mapa para proceder dentro del mundo del agente. Con una función `Q` precisa el agente podrá predecir mejor el impacto de sus decisiones y planificar sus acciones futuras, generando así una política cada vez mejor para interactuar con su mundo.\n",
        "\n",
        "## Pregunta 3:\n",
        "\n",
        "La implementación de dos redes neuronales para estimar la función `Q` es debido a que tener dos redes neuronales ayuda a evitar el sesgo de sobreestimación, lo que es común en modelos de RL basados en estimación de la función `Q`. Al entrenar se usan dos redes independientes y se actualizan los parámetros con el mínimo entre las estimaciones de ambas redes.\n",
        "Además al tener dos redes el entrenamiento se logra hacer más estable ya que las redes pueden irse compensando mutuamente.\n",
        "\n",
        "## Pregunta 4:\n",
        "\n",
        "Que un algoritmo de aprendizaje RL sea _model-free_ significa que no necesita un modelo explícito de su entorno para aprender a tomar decisiones. Es decir el agente aprende mediante sus interacciones con el entorno y no necesita conocer la función de recompensa.\n",
        "Que un algoritmo sea _off-policy_ quiere decir que puede aprender la política óptima utilizando experiencias que no fueron generadas por la política que está siendo actualmente aprendida. Es decir, puede aprender de un _buffer_ que fue generado por otra política.\n",
        "\n",
        "## Pregunta 5:\n",
        "\n",
        "El algoritmo SAC aborda el _trade-off_ entre exploración y explotación mediante el uso de **entropía**. La entropía mide el grado de incertidumbre de la política de decisión del agente. El algoritmo SAC incluye un termino de entropía en la función objetivo con el propósito de fomentar la exploración. Esta se pondera con un parámtro α, que aumenta o disminuye la tendencía a la exploración.\n",
        "\n",
        "## Pregunta 6:\n",
        "\n",
        "El algoritmo DQN está diseñado para operar en entornos discretos, con cantidad finita de acciones y conocida. Como el problema abordado involucra el control de un brazo robótico con dos grados de libertad, la cantidad de acciones es infinita, por lo que un algoritmo como DQN no serviría. Sería necesario discretizar las acciones del brazo, lo que podría llevar a errores en la precisión y estabilidad del robot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nWun2Dx1G-n"
      },
      "source": [
        "# Librerias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWo3hdToO23V"
      },
      "source": [
        "## PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G8SiBKSOO2Zi"
      },
      "outputs": [],
      "source": [
        "from torch.distributions.normal import Normal\n",
        "import torchvision.transforms as TF\n",
        "from collections import namedtuple\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy0kkBA_NSTt"
      },
      "source": [
        "## Librerias *comunes*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3u8D01lw1MLn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import gymnasium as gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI54X1JX7jIG"
      },
      "source": [
        "# Setup Reacher v4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "y2wrOBnS1fV4",
        "outputId": "89d83767-22ad-4580-ccfc-a7120d9b252b"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Reacher-v4\", render_mode =\"rgb_array\")\n",
        "\n",
        "observation, info = env.reset()\n",
        "\n",
        "# for _ in range(1000):\n",
        "#     action = env.action_space.sample()  # agent policy that uses the observation and info\n",
        "#     observation, reward, terminated, truncated, info = env.step(action)\n",
        "#     obs = env.render()\n",
        "\n",
        "#     if terminated or truncated:\n",
        "#         observation, info = env.reset()\n",
        "\n",
        "# env.close()\n",
        "# plt.imshow(obs)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEKmf6nNPHVn"
      },
      "source": [
        "# Aprendizaje Reforzado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No es estrictamente necesario realizar un pre-procesamiento de los estados del sistema, a diferencia de otros problemas en ML. Sin embargo, esto no descarta que pueda ser útil para el entrenamiento y la optimización realizar dicho proceso.\n",
        "Un tipo común de preprocesamiento que se peude considerar es normalizar los estados del sistema. Normalizar los estados puede ser útil para estandarizar la escala de las observaciones y hacer que el aprendizaje sea más eficiente. Se puede calcular la media y la desviación estándar de los estados observados durante un período de tiempo y luego usarlas para normalizar los estados en cada paso de tiempo. Esto puede ayudar a estabilizar el entrenamiento y mejorar la convergencia del modelo.\n",
        "\n",
        "Como se mencionó, en este caso el pre-procesamiento no es necesario debido a que el agente SAC ya es capaz de aprender usando los datos tal y como los entrega el `env` de `gymnasium`.\n",
        "\n",
        "En cuanto a la secuencialidad y la dinámica del sistema, estas están implícitas en la forma en que se diseñó el entorno Reacher-v4. El entorno tiene una representación interna del estado del sistema que evoluciona con el tiempo a medida que se toman acciones. La secuencialidad y la dinámica del sistema se reflejan en las observaciones que recibe tu agente en cada paso de tiempo y en cómo estas observaciones cambian en respuesta a las acciones tomadas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6kRCYirPZ19"
      },
      "source": [
        "## ReplayBuffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5_OBsyYrPHxU"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample_batch(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return {\n",
        "            'states': torch.tensor(np.array(states), dtype=torch.float32),\n",
        "            'actions': torch.tensor(np.array(actions), dtype=torch.float32),\n",
        "            'rewards': torch.tensor(np.array(rewards), dtype=torch.float32),\n",
        "            'next_states': torch.tensor(np.array(next_states), dtype=torch.float32),\n",
        "            'dones': torch.tensor(np.array(dones), dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "    def initialise_buffer(self, env, size):\n",
        "        state, _ = env.reset()\n",
        "        for _ in range(size):\n",
        "            action = env.action_space.sample()\n",
        "            next_state, reward, done, truncated, _ = env.step(action)\n",
        "            experience = (state, action, reward, next_state, done or truncated)\n",
        "            self.append(experience)\n",
        "            state = next_state\n",
        "            if done or truncated:\n",
        "                state, _ = env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SgGdp5DPrw_"
      },
      "source": [
        "## Actor Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TrJxCPN5Ps7R"
      },
      "outputs": [],
      "source": [
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, input_dims, num_actions, hidden_dims, name='actor'):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.input_dims = input_dims\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.num_actions = num_actions\n",
        "        self.name = name\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(input_dims, hidden_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims, hidden_dims),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.mu = nn.Sequential(\n",
        "            nn.Linear(hidden_dims, num_actions),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.sigma = nn.Sequential(\n",
        "            nn.Linear(hidden_dims, num_actions),\n",
        "            nn.Softplus()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.classifier(x)\n",
        "        mu = self.mu(x)\n",
        "        log_sigma = self.sigma(x)\n",
        "        return mu, log_sigma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPUVCKZgP20u"
      },
      "source": [
        "## Critic Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Z4WWzlUEP5AY"
      },
      "outputs": [],
      "source": [
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, input_dims, num_actions, output_dims, hidden_dims, name='critic'):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "        self.input_dims = input_dims\n",
        "        self.output_dims = output_dims\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.num_actions = num_actions\n",
        "        self.name = name\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(input_dims + num_actions, hidden_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims, hidden_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims, output_dims)\n",
        "        )\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], dim=1)\n",
        "        out = self.classifier(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHs5_sIiQI7q"
      },
      "source": [
        "## SAC Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def save_checkpoint(agent, episode, checkpoint_dir='checkpoints'):\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "    torch.save(agent.critic1.state_dict(), os.path.join(checkpoint_dir, f'critic1_{episode}.pth'))\n",
        "    torch.save(agent.critic2.state_dict(), os.path.join(checkpoint_dir, f'critic2_{episode}.pth'))\n",
        "    torch.save(agent.target_critic1.state_dict(), os.path.join(checkpoint_dir, f'target_critic1_{episode}.pth'))\n",
        "    torch.save(agent.target_critic2.state_dict(), os.path.join(checkpoint_dir, f'target_critic2_{episode}.pth'))\n",
        "    torch.save(agent.actor.state_dict(), os.path.join(checkpoint_dir, f'actor_{episode}.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JouqABg5QKd0"
      },
      "outputs": [],
      "source": [
        "Rollout = namedtuple('Rollout', ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "class SACAgent():\n",
        "    def __init__(self, env, lr, gamma, soft_update_tau, memory_size, hidden_size, alpha=0.2, log_std_range=[-20,2]):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.env = env\n",
        "        self.n_states = self.env.observation_space.shape[0]\n",
        "        self.n_actions = self.env.action_space.shape[0]\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.tau = soft_update_tau\n",
        "        self.hidden_size = hidden_size\n",
        "        self.min_clamp = log_std_range[0]\n",
        "        self.max_clamp = log_std_range[-1]\n",
        "\n",
        "        self._init_model()\n",
        "        self.update_target_networks(tau=1)\n",
        "\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        self.memory = ReplayBuffer(capacity=memory_size)\n",
        "\n",
        "    def _init_model(self):\n",
        "        self.critic1 = CriticNetwork(self.n_states, self.n_actions, 1, self.hidden_size).to(self.device)\n",
        "        self.critic2 = CriticNetwork(self.n_states, self.n_actions, 1, self.hidden_size).to(self.device)\n",
        "        self.target_critic1 = CriticNetwork(self.n_states, self.n_actions, 1, self.hidden_size).to(self.device)\n",
        "        self.target_critic2 = CriticNetwork(self.n_states, self.n_actions, 1, self.hidden_size).to(self.device)\n",
        "        self.actor = ActorNetwork(self.n_states, self.n_actions, self.hidden_size).to(self.device)\n",
        "\n",
        "        self.critic_optim1 = optim.Adam(self.critic1.parameters(), lr=self.lr)\n",
        "        self.critic_optim2 = optim.Adam(self.critic2.parameters(), lr=self.lr)\n",
        "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=self.lr)\n",
        "\n",
        "    def update_target_networks(self, tau=None):\n",
        "        tau = self.tau if tau is None else tau\n",
        "        for target_param, param in zip(self.target_critic1.parameters(), self.critic1.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        for target_param, param in zip(self.target_critic2.parameters(), self.critic2.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
        "        mean, log_sigma = self.actor(state)\n",
        "        sigma = log_sigma.exp().clamp(self.min_clamp, self.max_clamp)\n",
        "        normal = torch.distributions.Normal(mean, sigma)\n",
        "        z = normal.rsample()\n",
        "        action = torch.tanh(z)\n",
        "        log_prob = normal.log_prob(z) - torch.log(1 - action**2 + 1e-7)\n",
        "        log_prob = log_prob.sum(-1, keepdim=True)\n",
        "        return action, log_prob\n",
        "\n",
        "    def critic_loss(self, samples):\n",
        "        states = samples['states'].to(self.device)\n",
        "        actions = samples['actions'].to(self.device)\n",
        "        rewards = samples['rewards'].to(self.device)\n",
        "        next_states = samples['next_states'].to(self.device)\n",
        "        dones = samples['dones'].to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_actions, next_log_probs = self.get_action(next_states)\n",
        "            next_q1 = self.target_critic1(next_states, next_actions)\n",
        "            next_q2 = self.target_critic2(next_states, next_actions)\n",
        "            next_q = torch.min(next_q1, next_q2) - self.alpha * next_log_probs\n",
        "            target_q = rewards + (self.gamma * (1 - dones) * next_q)\n",
        "\n",
        "        pred_q1 = self.critic1(states, actions)\n",
        "        pred_q2 = self.critic2(states, actions)\n",
        "\n",
        "        loss1 = self.criterion(pred_q1, target_q.detach())\n",
        "        loss2 = self.criterion(pred_q2, target_q.detach())\n",
        "\n",
        "        return loss1, loss2\n",
        "\n",
        "    def actor_loss(self, states):\n",
        "        actions, log_probs = self.get_action(states)\n",
        "        q1 = self.critic1(states, actions)\n",
        "        q2 = self.critic2(states, actions)\n",
        "        q = torch.min(q1, q2)\n",
        "\n",
        "        policy_loss = (self.alpha * log_probs - q).mean()\n",
        "        return policy_loss, log_probs\n",
        "\n",
        "    def _choose_action(self, state, random=False):\n",
        "        if random:\n",
        "            actions = self.env.action_space.sample()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                actions, _ = self.get_action(state)\n",
        "        return actions\n",
        "\n",
        "    def learn(self, samples, target_update=False):\n",
        "        critic_loss1, critic_loss2 = self.critic_loss(samples)\n",
        "        self.critic_optim1.zero_grad()\n",
        "        self.critic_optim2.zero_grad()\n",
        "        critic_loss1.backward()\n",
        "        critic_loss2.backward()\n",
        "        self.critic_optim1.step()\n",
        "        self.critic_optim2.step()\n",
        "\n",
        "        actor_loss, _ = self.actor_loss(samples['states'].to(self.device))\n",
        "        self.actor_optim.zero_grad()\n",
        "        actor_loss.backward()\n",
        "\n",
        "        for name, param in self.actor.named_parameters():\n",
        "            if param.grad is None:\n",
        "                print(f'No gradient for {name}')\n",
        "\n",
        "        self.actor_optim.step()\n",
        "\n",
        "        if target_update:\n",
        "            self.update_target_networks()\n",
        "        return critic_loss1.item(), critic_loss2.item(), actor_loss.item()\n",
        "\n",
        "    def train(self, n_episode=1000, batch_size=64, report_freq=10,\n",
        "              checkpoint_freq=50, initial_memory=1024, target_actualizations=100):\n",
        "        self.memory.initialise_buffer(self.env, size=initial_memory)\n",
        "        results = []\n",
        "        for i in range(n_episode):\n",
        "            state, _ = self.env.reset()\n",
        "            done, truncated = False, False\n",
        "            eps_reward = 0\n",
        "            steps = 0\n",
        "\n",
        "            while not (done or truncated):\n",
        "                action = self._choose_action(state)\n",
        "                action = action.detach().cpu().numpy()\n",
        "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
        "                roll = Rollout(state, action, reward, next_state, done or truncated)\n",
        "                self.memory.append(roll)\n",
        "                state = next_state\n",
        "\n",
        "                samples = self.memory.sample_batch(batch_size)\n",
        "                update_target_networks = (i % np.ceil(n_episode / target_actualizations) == 0)\n",
        "                critic_loss1, critic_loss2, actor_loss = self.learn(samples, target_update=update_target_networks)\n",
        "\n",
        "                eps_reward += reward\n",
        "                steps += 1\n",
        "\n",
        "            results.append(eps_reward)\n",
        "\n",
        "            if i % report_freq == 0:\n",
        "                print(f'Episode {i}/{n_episode} \\t Reward: {eps_reward:.4f} \\t Critic Loss: {critic_loss1:.3f} \\t Actor Loss: {actor_loss:.3f}')\n",
        "\n",
        "            if i % checkpoint_freq == 0:\n",
        "                save_checkpoint(self, i)\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTirYTDFQXJL"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TdVL-X9dQYYH",
        "outputId": "1773989f-e7a0-4ac8-8974-2a63f5517fab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(11,)\n",
            "(2,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Bruno\\AppData\\Local\\Temp\\ipykernel_13780\\348774137.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  state = torch.tensor(state, dtype=torch.float32).to(self.device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0/1000 \t Reward: -61.5931 \t Critic Loss: 0.439 \t Actor Loss: 0.130\n",
            "Episode 10/1000 \t Reward: -45.3458 \t Critic Loss: 0.223 \t Actor Loss: 0.006\n",
            "Episode 20/1000 \t Reward: -46.5470 \t Critic Loss: 0.235 \t Actor Loss: 0.041\n",
            "Episode 30/1000 \t Reward: -50.0572 \t Critic Loss: 0.242 \t Actor Loss: 0.149\n",
            "Episode 40/1000 \t Reward: -52.4593 \t Critic Loss: 0.251 \t Actor Loss: 0.188\n",
            "Episode 50/1000 \t Reward: -54.7397 \t Critic Loss: 0.239 \t Actor Loss: 0.227\n",
            "Episode 60/1000 \t Reward: -49.3942 \t Critic Loss: 0.231 \t Actor Loss: 0.313\n",
            "Episode 70/1000 \t Reward: -41.4976 \t Critic Loss: 0.226 \t Actor Loss: 0.378\n",
            "Episode 80/1000 \t Reward: -52.4545 \t Critic Loss: 0.228 \t Actor Loss: 0.438\n",
            "Episode 90/1000 \t Reward: -53.0727 \t Critic Loss: 0.242 \t Actor Loss: 0.483\n",
            "Episode 100/1000 \t Reward: -50.7237 \t Critic Loss: 0.236 \t Actor Loss: 0.535\n",
            "Episode 110/1000 \t Reward: -52.1891 \t Critic Loss: 0.271 \t Actor Loss: 0.593\n",
            "Episode 120/1000 \t Reward: -56.5967 \t Critic Loss: 0.259 \t Actor Loss: 0.746\n",
            "Episode 130/1000 \t Reward: -56.0584 \t Critic Loss: 0.256 \t Actor Loss: 0.786\n",
            "Episode 140/1000 \t Reward: -49.4444 \t Critic Loss: 0.258 \t Actor Loss: 0.875\n",
            "Episode 150/1000 \t Reward: -43.9013 \t Critic Loss: 0.273 \t Actor Loss: 0.903\n",
            "Episode 160/1000 \t Reward: -44.9420 \t Critic Loss: 0.235 \t Actor Loss: 1.021\n",
            "Episode 170/1000 \t Reward: -40.3637 \t Critic Loss: 0.251 \t Actor Loss: 1.056\n",
            "Episode 180/1000 \t Reward: -47.6834 \t Critic Loss: 0.252 \t Actor Loss: 1.165\n",
            "Episode 190/1000 \t Reward: -50.0445 \t Critic Loss: 0.263 \t Actor Loss: 1.173\n",
            "Episode 200/1000 \t Reward: -44.8845 \t Critic Loss: 0.249 \t Actor Loss: 1.244\n",
            "Episode 210/1000 \t Reward: -52.4571 \t Critic Loss: 0.310 \t Actor Loss: 1.295\n",
            "Episode 220/1000 \t Reward: -55.6257 \t Critic Loss: 0.290 \t Actor Loss: 1.460\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m agent \u001b[38;5;241m=\u001b[39m SACAgent(env, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m, soft_update_tau\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m     12\u001b[0m                  memory_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Entrena al agente\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m learning_data \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Grafica los resultados\u001b[39;00m\n\u001b[0;32m     18\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(learning_data, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReward over episodes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[1;32mIn[21], line 137\u001b[0m, in \u001b[0;36mSACAgent.train\u001b[1;34m(self, n_episode, batch_size, report_freq, checkpoint_freq, initial_memory, target_actualizations)\u001b[0m\n\u001b[0;32m    135\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msample_batch(batch_size)\n\u001b[0;32m    136\u001b[0m update_target_networks \u001b[38;5;241m=\u001b[39m (i \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39mceil(n_episode \u001b[38;5;241m/\u001b[39m target_actualizations) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 137\u001b[0m critic_loss1, critic_loss2, actor_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_update\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_target_networks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m eps_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m    140\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "Cell \u001b[1;32mIn[21], line 96\u001b[0m, in \u001b[0;36mSACAgent.learn\u001b[1;34m(self, samples, target_update)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\u001b[38;5;28mself\u001b[39m, samples, target_update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     95\u001b[0m     critic_loss1, critic_loss2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_loss(samples)\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_optim1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optim2\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     98\u001b[0m     critic_loss1\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[1;32mc:\\Users\\Bruno\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Bruno\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_dynamo\\decorators.py:50\u001b[0m, in \u001b[0;36mdisable\u001b[1;34m(fn, recursive)\u001b[0m\n\u001b[0;32m     48\u001b[0m         fn \u001b[38;5;241m=\u001b[39m innermost_fn(fn)\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn)\n\u001b[1;32m---> 50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDisableContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DisableContext()\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Bruno\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:406\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn)\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetsourcefile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Bruno\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\inspect.py:949\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m(filename\u001b[38;5;241m.\u001b[39mendswith(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m    947\u001b[0m              importlib\u001b[38;5;241m.\u001b[39mmachinery\u001b[38;5;241m.\u001b[39mEXTENSION_SUFFIXES):\n\u001b[0;32m    948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 949\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filename\n\u001b[0;32m    951\u001b[0m \u001b[38;5;66;03m# only return a non-existent filename if the module has a PEP 302 loader\u001b[39;00m\n",
            "File \u001b[1;32m<frozen genericpath>:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Define el nombre del entorno y el número de episodios\n",
        "env_name = \"Reacher-v4\"\n",
        "epos = 1000\n",
        "\n",
        "# Crea el entorno\n",
        "env = gym.make(env_name)\n",
        "print(np.shape(env.observation_space.sample()))\n",
        "print(np.shape(env.action_space.sample()))\n",
        "\n",
        "# Inicializa el agente SAC con los parámetros apropiados\n",
        "agent = SACAgent(env, lr=0.0001, gamma=0.99, soft_update_tau=0.005, alpha=0.5,\n",
        "                 memory_size=10000, hidden_size=256)\n",
        "\n",
        "# Entrena al agente\n",
        "learning_data = agent.train(n_episode=epos, batch_size=256, report_freq=10, checkpoint_freq=100)\n",
        "\n",
        "# Grafica los resultados\n",
        "plt.plot(learning_data, label='Reward over episodes')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.title('SAC Training on Reacher-v4')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Cierra el entorno\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
